import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const supabase = createClient(
  Deno.env.get('SUPABASE_URL') ?? '',
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
);

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { message, conversationId, productCategory } = await req.json();
    
    console.log(`ü§ñ Generating intelligent response for category: ${productCategory}`);

    // Buscar dados da conversa
    const { data: conversation, error: convError } = await supabase
      .from('conversations')
      .select('*')
      .eq('id', conversationId)
      .single();

    if (convError || !conversation) {
      throw new Error('Conversation not found');
    }

    // Determinar qual agente usar
    const { data: respondingAgentId } = await supabase.rpc('get_responding_agent', {
      conversation_uuid: conversationId
    });

    if (!respondingAgentId) {
      throw new Error('No responding agent found');
    }

    // Buscar configura√ß√£o do agente
    const { data: agent, error: agentError } = await supabase
      .from('agent_configs')
      .select('*')
      .eq('id', respondingAgentId)
      .single();

    if (agentError || !agent) {
      throw new Error('Agent configuration not found');
    }

    // Buscar hist√≥rico da conversa
    const { data: messages } = await supabase
      .from('messages')
      .select('content, sender_type, created_at')
      .eq('conversation_id', conversationId)
      .order('created_at', { ascending: true })
      .limit(20);

    const conversationHistory = messages
      ?.map(msg => `${msg.sender_type === 'customer' ? 'Cliente' : 'Atendente'}: ${msg.content}`)
      .join('\n') || '';

    // Buscar contextos extra√≠dos
    const { data: extractedContexts } = await supabase
      .from('extracted_contexts')
      .select('context_type, context_data')
      .eq('conversation_id', conversationId)
      .eq('is_active', true);

    const contextInfo = extractedContexts?.map(ctx => 
      `${ctx.context_type}: ${JSON.stringify(ctx.context_data)}`
    ).join('\n') || '';

    // Buscar conhecimento relevante se for agente especializado
    let relevantKnowledge = '';
    if (agent.agent_type === 'specialist' && agent.product_category) {
      const { data: knowledgeChunks } = await supabase.rpc('search_knowledge_chunks', {
        query_embedding: await generateEmbedding(message),
        target_agent_category: agent.product_category,
        similarity_threshold: 0.7,
        max_results: 3
      });

      if (knowledgeChunks && knowledgeChunks.length > 0) {
        relevantKnowledge = knowledgeChunks
          .map(chunk => `Conhecimento: ${chunk.content}`)
          .join('\n\n');
      }
    }

    // Construir prompt final
    let finalPrompt = agent.system_prompt;
    
    if (contextInfo) {
      finalPrompt += `\n\nInforma√ß√µes do cliente:\n${contextInfo}`;
    }
    
    if (relevantKnowledge) {
      finalPrompt += `\n\nConhecimento relevante:\n${relevantKnowledge}`;
    }
    
    if (conversationHistory) {
      finalPrompt += `\n\nHist√≥rico da conversa:\n${conversationHistory}`;
    }
    
    finalPrompt += `\n\nMensagem atual do cliente: "${message}"`;
    finalPrompt += `\n\nResponda de forma natural, √∫til e profissional.`;

    // Gerar resposta usando a API do LLM configurado
    const response = await callLLMAPI(
      agent.llm_model || 'claude-3-5-sonnet-20241022',
      finalPrompt,
      agent.temperature || 0.7,
      agent.max_tokens || 500
    );

    // Salvar a resposta no banco
    const { data: messageData } = await supabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content: response,
        sender_type: 'bot',
        agent_id: agent.id,
        agent_type: agent.agent_type,
        status: 'sent'
      })
      .select()
      .single();

    // Atualizar conversa
    await supabase
      .from('conversations')
      .update({
        current_agent_id: agent.id,
        last_message_at: new Date().toISOString()
      })
      .eq('id', conversationId);

    console.log(`‚úÖ Response generated by ${agent.agent_name}: "${response.substring(0, 100)}..."`);

    return new Response(JSON.stringify({
      response,
      agentName: agent.agent_name,
      agentType: agent.agent_type,
      messageId: messageData.id
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });

  } catch (error) {
    console.error('Error generating intelligent response:', error);
    
    await supabase.from('system_logs').insert({
      level: 'error',
      source: 'intelligent-agent-response',
      message: 'Failed to generate response',
      data: { error: error.message }
    });

    return new Response(JSON.stringify({ 
      error: error.message,
      response: 'Desculpe, ocorreu um erro. Um especialista entrar√° em contato em breve.'
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});

// Fun√ß√£o para chamar APIs de LLM
async function callLLMAPI(
  model: string,
  prompt: string,
  temperature: number = 0.7,
  maxTokens: number = 500
): Promise<string> {
  let apiKey = '';
  let apiUrl = '';
  let headers = {};
  let requestBody = {};

  if (model.startsWith('claude')) {
    apiKey = Deno.env.get('ANTHROPIC_API_KEY');
    if (!apiKey) throw new Error('Anthropic API key not configured');
    
    apiUrl = 'https://api.anthropic.com/v1/messages';
    headers = {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01'
    };
    requestBody = {
      model,
      max_tokens: maxTokens,
      temperature,
      messages: [{ role: 'user', content: prompt }]
    };
  } else if (model.startsWith('gpt')) {
    apiKey = Deno.env.get('OPENAI_API_KEY');
    if (!apiKey) throw new Error('OpenAI API key not configured');
    
    apiUrl = 'https://api.openai.com/v1/chat/completions';
    headers = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    };

    // Use max_completion_tokens for newer models
    if (model.includes('gpt-5') || model.includes('gpt-4.1') || model.includes('o3') || model.includes('o4')) {
      requestBody = {
        model,
        messages: [{ role: 'user', content: prompt }],
        max_completion_tokens: maxTokens
      };
    } else {
      requestBody = {
        model,
        messages: [{ role: 'user', content: prompt }],
        max_tokens: maxTokens,
        temperature
      };
    }
  } else if (model.startsWith('grok')) {
    apiKey = Deno.env.get('XAI_API_KEY');
    if (!apiKey) throw new Error('xAI API key not configured');
    
    apiUrl = 'https://api.x.ai/v1/chat/completions';
    headers = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    };
    requestBody = {
      model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: maxTokens,
      temperature
    };
  } else {
    throw new Error(`Unsupported LLM model: ${model}`);
  }

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(requestBody)
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error('LLM API error:', errorText);
    throw new Error(`LLM API error: ${response.status}`);
  }

  const data = await response.json();
  
  // Extract response based on LLM type
  if (model.startsWith('claude')) {
    return data.content[0].text;
  } else if (model.startsWith('gpt') || model.startsWith('grok')) {
    return data.choices[0].message.content;
  }
  
  throw new Error('Unable to extract response from LLM');
}

// Fun√ß√£o para gerar embedding (simplificada - usar OpenAI)
async function generateEmbedding(text: string): Promise<number[]> {
  const apiKey = Deno.env.get('OPENAI_API_KEY');
  if (!apiKey) {
    return []; // Retornar array vazio se n√£o houver API key
  }

  try {
    const response = await fetch('https://api.openai.com/v1/embeddings', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        model: 'text-embedding-ada-002',
        input: text
      })
    });

    if (!response.ok) {
      throw new Error('Failed to generate embedding');
    }

    const data = await response.json();
    return data.data[0].embedding;
  } catch (error) {
    console.error('Error generating embedding:', error);
    return [];
  }
}